2020-04-02 09:57:58.858 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 13540 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 09:57:58.999 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 09:57:59.015 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 09:58:01.889 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 09:58:03.904 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 09:58:03.904 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 09:58:03.904 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 09:58:03.904 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 09:58:04.092 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 09:58:04.092 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 09:58:04.092 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 09:58:04.310 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 09:58:04.310 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 09:58:04.342 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 09:58:04.342 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 09:58:04.342 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 09:58:04.342 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 09:58:04.795 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 09:58:04.795 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 09:58:05.154 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 09:58:05.154 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 09:58:05.154 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 09:58:05.451 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 09:58:05.451 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 09:58:05.466 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 09:58:05.654 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 45
2020-04-02 09:58:05.654 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 09:58:06.060 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============接受到消息了=============
2020-04-02 09:58:06.091 [ Thread-2 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 137 ] - ===把byte反序列化成对象异常===
java.io.StreamCorruptedException: invalid stream header: 66647364
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:866)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:358)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:132)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 09:58:06.091 [ Thread-2 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 91 ] - ===消费消息出错啦，德玛西亚！！！===
java.lang.NullPointerException: null
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:143)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 09:58:08.887 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 11.684 seconds (JVM running for 17.526)
2020-04-02 09:58:09.059 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 09:58:09.122 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions [test0-0]
2020-04-02 09:58:09.122 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 09:58:09.247 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 46
2020-04-02 09:58:09.247 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 09:58:09.247 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 46
2020-04-02 09:58:09.247 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 09:58:09.668 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:20:19.035 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 2552 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 10:20:19.037 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 10:20:19.038 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 10:20:19.947 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 10:20:21.197 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:20:21.197 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:20:21.197 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:20:21.199 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:20:21.225 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:20:21.226 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 10:20:21.227 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:20:21.446 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:20:21.446 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:20:27.051 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:20:27.052 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:20:38.895 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:20:38.896 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:22:17.821 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:22:17.821 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:22:17.825 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:22:17.826 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:22:17.827 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:22:17.828 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:22:17.832 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:22:17.833 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:22:18.711 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:22:19.140 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 49
2020-04-02 10:22:19.141 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 10:22:19.144 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 49
2020-04-02 10:22:19.146 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:22:20.755 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 122.389 seconds (JVM running for 125.592)
2020-04-02 10:22:21.684 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:24:46.535 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 12684 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 10:24:46.540 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 10:24:46.540 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 10:24:47.499 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 10:24:48.604 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:24:48.604 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:24:48.606 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:24:48.608 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:24:48.631 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:24:48.634 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:24:48.643 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 10:24:48.705 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:24:48.706 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:24:48.727 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:24:48.728 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:24:48.736 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:24:48.737 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:24:49.139 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:24:49.142 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:24:49.145 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:24:49.145 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:24:49.177 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:24:49.178 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:24:49.179 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:24:49.179 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:24:49.383 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:24:50.273 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 52
2020-04-02 10:24:50.273 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 10:24:50.328 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 52
2020-04-02 10:24:50.330 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:24:51.627 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.725 seconds (JVM running for 8.4)
2020-04-02 10:24:52.278 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:37:09.553 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 16116 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 10:37:09.556 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 10:37:09.557 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 10:37:10.562 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 10:37:11.914 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:37:11.915 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:37:11.916 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:37:11.916 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:37:11.941 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:37:11.942 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:37:11.943 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 10:37:12.016 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:37:12.016 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:37:12.038 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:37:12.038 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:37:12.041 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:37:12.041 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:37:12.425 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:37:12.429 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:37:12.432 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:37:12.432 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:37:12.594 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 54
2020-04-02 10:37:12.596 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:37:12.698 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:37:12.699 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:37:12.700 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:37:12.700 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:37:13.047 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============接受到消息了=============
2020-04-02 10:37:13.052 [ Thread-2 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 137 ] - ===把byte反序列化成对象异常===
java.io.StreamCorruptedException: invalid stream header: 31313132
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:866)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:358)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:132)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 10:37:13.053 [ Thread-2 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 91 ] - ===消费消息出错啦，德玛西亚！！！===
java.lang.NullPointerException: null
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:143)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 10:37:15.211 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 6.553 seconds (JVM running for 9.145)
2020-04-02 10:37:15.478 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 10:37:15.484 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:37:15.484 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:37:15.636 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 10:37:15.679 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions [test0-0]
2020-04-02 10:37:15.679 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:37:15.764 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 55
2020-04-02 10:37:15.765 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 10:37:15.768 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 55
2020-04-02 10:37:15.769 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:37:15.885 [ kafka-producer-network-thread | producer-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:38:19.980 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 6944 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 10:38:19.982 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 10:38:19.983 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 10:38:20.956 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 10:38:22.015 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:38:22.016 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:38:22.016 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:38:22.016 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:38:22.043 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:38:22.044 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:38:22.045 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 10:38:22.112 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:38:22.112 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:38:22.131 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:38:22.131 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:38:22.132 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:38:22.132 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:38:22.439 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:38:22.442 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:38:22.444 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:38:22.445 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:38:22.446 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:38:22.447 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:38:22.449 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:38:22.450 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:38:22.617 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:38:22.702 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 58
2020-04-02 10:38:22.703 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 10:38:22.707 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 58
2020-04-02 10:38:22.708 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:38:25.013 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 5.833 seconds (JVM running for 8.449)
2020-04-02 10:38:25.258 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 10:38:25.264 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:38:25.265 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:38:25.402 [ kafka-producer-network-thread | producer-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:39:50.759 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 5264 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 10:39:50.761 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 10:39:50.761 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 10:39:51.729 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 10:39:52.779 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:39:52.779 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:39:52.780 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:39:52.782 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:39:52.805 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:39:52.807 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:39:52.807 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 10:39:52.867 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:39:52.868 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:39:52.902 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:39:52.903 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:39:52.908 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:39:52.908 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:39:53.184 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:39:53.187 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:39:53.190 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:39:53.190 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:39:53.202 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:39:53.203 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:39:53.204 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:39:53.205 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:39:53.344 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 60
2020-04-02 10:39:53.346 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:39:55.954 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 5.982 seconds (JVM running for 8.708)
2020-04-02 10:39:56.326 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9091]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 10:39:56.332 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:39:56.332 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:39:56.388 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 10:39:56.450 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions [test0-0]
2020-04-02 10:39:56.450 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:39:56.538 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 61
2020-04-02 10:39:56.539 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:39:56.539 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 61
2020-04-02 10:39:56.540 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 10:40:17.348 [ kafka-producer-network-thread | producer-2 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Producer clientId=producer-2] Connection to node -1 could not be established. Broker may not be available.
2020-04-02 10:40:38.405 [ kafka-producer-network-thread | producer-2 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Producer clientId=producer-2] Connection to node -1 could not be established. Broker may not be available.
2020-04-02 10:41:01.407 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 2144 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 10:41:01.409 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 10:41:01.410 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 10:41:02.326 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 10:41:03.397 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:41:03.397 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:41:03.399 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:41:03.416 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:41:03.425 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:41:03.424 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:41:03.430 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 10:41:03.492 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:41:03.493 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:41:03.526 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:41:03.526 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:41:03.535 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:41:03.536 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:41:03.845 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:41:03.846 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:41:03.849 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:41:03.852 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:41:03.853 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:41:03.850 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:41:03.858 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:41:03.859 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:41:04.028 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:41:04.126 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 64
2020-04-02 10:41:04.128 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 64
2020-04-02 10:41:04.128 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 10:41:04.128 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:41:06.383 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 5.65 seconds (JVM running for 8.27)
2020-04-02 10:41:06.657 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 10:41:06.663 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:41:06.663 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:41:06.797 [ kafka-producer-network-thread | producer-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:43:23.303 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 8956 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 10:43:23.305 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 10:43:23.306 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 10:43:24.238 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 10:43:25.442 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:43:25.443 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:43:25.443 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:43:25.445 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:43:25.469 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:43:25.469 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:43:25.469 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 10:43:25.541 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:43:25.542 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:43:25.557 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:43:25.557 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:43:25.559 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:43:25.560 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:43:26.144 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:43:26.144 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:43:26.148 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:43:26.148 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:43:26.150 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:43:26.151 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:43:26.152 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:43:26.152 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:43:26.972 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 66
2020-04-02 10:43:26.972 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 66
2020-04-02 10:43:26.974 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:43:26.974 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 10:43:28.618 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 6.146 seconds (JVM running for 8.849)
2020-04-02 10:43:28.903 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 10:43:28.910 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:43:28.911 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:43:29.100 [ kafka-producer-network-thread | producer-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:43:29.295 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============接受到消息了=============
2020-04-02 10:43:29.301 [ Thread-3 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 137 ] - ===把byte反序列化成对象异常===
java.io.StreamCorruptedException: invalid stream header: 32323232
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:866)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:358)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:132)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 10:43:29.302 [ Thread-3 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 91 ] - ===消费消息出错啦，德玛西亚！！！===
java.lang.NullPointerException: null
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:143)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 10:48:14.283 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 7880 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 10:48:14.285 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 10:48:14.286 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 10:48:15.416 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 10:48:16.499 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:48:16.500 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:48:16.500 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 10:48:16.502 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 10:48:16.526 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:48:16.526 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 10:48:16.526 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 10:48:16.591 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:48:16.591 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:48:16.628 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:48:16.629 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:48:16.633 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 10:48:16.634 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 10:48:17.921 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:48:17.924 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:48:17.927 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:48:17.927 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:48:18.369 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 68
2020-04-02 10:48:18.372 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:48:18.736 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 10:48:18.736 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 10:48:18.737 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 10:48:18.737 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:48:18.840 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============接受到消息了=============
2020-04-02 10:48:18.845 [ Thread-3 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 137 ] - ===把byte反序列化成对象异常===
java.io.StreamCorruptedException: invalid stream header: 32323232
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:866)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:358)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:132)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 10:48:18.846 [ Thread-3 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 91 ] - ===消费消息出错啦，德玛西亚！！！===
java.lang.NullPointerException: null
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:143)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 10:48:19.960 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.363 seconds (JVM running for 9.09)
2020-04-02 10:48:21.416 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 10:48:21.459 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions [test0-0]
2020-04-02 10:48:21.460 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 10:48:21.551 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 69
2020-04-02 10:48:21.552 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 10:48:21.551 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 69
2020-04-02 10:48:21.555 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 10:48:21.625 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:02:40.188 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 5224 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 11:02:40.189 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 11:02:40.190 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 11:02:41.063 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 11:02:42.074 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:02:42.074 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:02:42.075 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:02:42.075 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:02:42.101 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:02:42.100 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 11:02:42.103 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:02:42.168 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:02:42.168 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:02:42.199 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:02:42.199 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:02:42.220 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:02:42.221 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:02:42.627 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:02:42.630 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:02:42.632 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:02:42.632 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:02:42.636 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:02:42.637 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:02:42.638 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:02:42.638 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:02:42.806 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:02:42.882 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 72
2020-04-02 11:02:42.882 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 11:02:42.888 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 72
2020-04-02 11:02:42.889 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 11:02:43.109 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============接受到消息了=============
2020-04-02 11:02:43.115 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 165 ] - 执行消息serviceBeanName=sendMailService,methodName=sendMail
2020-04-02 11:02:43.115 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.service.impl.SendMailServiceImpl : 34 ] - =====sendMail===== in params={subject=峡谷之巅见, receiveAccount=1209414113@qq.com, text=德玛西亚万岁！}
2020-04-02 11:02:43.469 [ Thread-3 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 176 ] - ===通过反射执行方法出错！！！===
java.lang.reflect.InvocationTargetException: null
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.execute(KafkaConsumerRunner.java:170)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:80)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.springframework.dao.InvalidDataAccessApiUsageException: NOAUTH Authentication required.; nested exception is redis.clients.jedis.exceptions.JedisDataException: NOAUTH Authentication required.
	at org.springframework.data.redis.connection.jedis.JedisExceptionConverter.convert(JedisExceptionConverter.java:64)
	at org.springframework.data.redis.connection.jedis.JedisExceptionConverter.convert(JedisExceptionConverter.java:41)
	at org.springframework.data.redis.PassThroughExceptionTranslationStrategy.translate(PassThroughExceptionTranslationStrategy.java:44)
	at org.springframework.data.redis.FallbackExceptionTranslationStrategy.translate(FallbackExceptionTranslationStrategy.java:42)
	at org.springframework.data.redis.connection.jedis.JedisConnection.convertJedisAccessException(JedisConnection.java:142)
	at org.springframework.data.redis.connection.jedis.JedisStringCommands.convertJedisAccessException(JedisStringCommands.java:815)
	at org.springframework.data.redis.connection.jedis.JedisStringCommands.get(JedisStringCommands.java:68)
	at org.springframework.data.redis.connection.DefaultedRedisConnection.get(DefaultedRedisConnection.java:253)
	at org.springframework.data.redis.core.DefaultValueOperations$1.inRedis(DefaultValueOperations.java:57)
	at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:59)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:224)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:184)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:95)
	at org.springframework.data.redis.core.DefaultValueOperations.get(DefaultValueOperations.java:53)
	at com.lzycompany.messagequeue.common.util.RedisUtil.get(RedisUtil.java:94)
	at com.lzycompany.messagequeue.server.service.impl.SendMailServiceImpl.sendMail(SendMailServiceImpl.java:35)
	... 7 common frames omitted
Caused by: redis.clients.jedis.exceptions.JedisDataException: NOAUTH Authentication required.
	at redis.clients.jedis.Protocol.processError(Protocol.java:130)
	at redis.clients.jedis.Protocol.process(Protocol.java:164)
	at redis.clients.jedis.Protocol.read(Protocol.java:218)
	at redis.clients.jedis.Connection.readProtocolWithCheckingBroken(Connection.java:341)
	at redis.clients.jedis.Connection.getBinaryBulkReply(Connection.java:260)
	at redis.clients.jedis.BinaryJedis.get(BinaryJedis.java:246)
	at org.springframework.data.redis.connection.jedis.JedisStringCommands.get(JedisStringCommands.java:66)
	... 16 common frames omitted
2020-04-02 11:02:45.603 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.042 seconds (JVM running for 8.646)
2020-04-02 11:02:46.076 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:04:01.356 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 16444 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 11:04:01.357 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 11:04:01.358 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 11:04:02.397 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 11:04:03.464 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:04:03.464 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:04:03.465 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:04:03.466 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:04:03.490 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:04:03.491 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:04:03.493 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 11:04:03.557 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:04:03.558 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:04:03.587 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:04:03.588 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:04:03.596 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:04:03.597 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:04:04.195 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:04:04.196 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:04:04.199 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:04:04.200 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:04:04.201 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:04:04.201 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:04:04.204 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:04:04.204 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:04:04.648 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:04:04.871 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 75
2020-04-02 11:04:04.872 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 11:04:04.873 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 75
2020-04-02 11:04:04.875 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 11:04:06.453 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.753 seconds (JVM running for 8.867)
2020-04-02 11:04:07.156 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:15:12.306 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 3312 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 11:15:12.340 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 11:15:12.341 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 11:15:13.405 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 11:15:14.582 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:15:14.582 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:15:14.583 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:15:14.606 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:15:14.612 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:15:14.618 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:15:14.612 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 11:15:14.689 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:15:14.689 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:15:14.732 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:15:14.733 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:15:14.735 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:15:14.736 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:15:15.336 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:15:15.339 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:15:15.341 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:15:15.341 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:15:15.546 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 77
2020-04-02 11:15:15.548 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 11:15:16.040 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:15:16.041 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:15:16.041 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:15:16.042 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:15:17.538 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.795 seconds (JVM running for 8.357)
2020-04-02 11:15:17.932 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:15:52.608 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 2196 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 11:15:52.609 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 11:15:52.610 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 11:15:53.636 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 11:15:54.720 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:15:54.720 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:15:54.721 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:15:54.723 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:15:54.751 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:15:54.751 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:15:54.752 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 11:15:54.832 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:15:54.833 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:15:54.845 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:15:54.845 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:15:54.855 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:15:54.855 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:15:55.145 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:15:55.146 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:15:55.150 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:15:55.150 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:15:55.152 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:15:55.153 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:15:55.152 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:15:55.154 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:15:57.891 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.013 seconds (JVM running for 9.022)
2020-04-02 11:15:58.262 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:16:37.658 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 4648 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 11:16:37.659 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 11:16:37.660 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 11:16:38.531 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 11:16:39.607 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:16:39.607 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:16:39.607 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:16:39.608 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:16:39.633 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:16:39.634 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 11:16:39.634 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:16:39.693 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:16:39.693 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:16:39.719 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:16:39.719 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:16:39.724 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:16:39.724 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:16:40.046 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:16:40.049 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:16:40.049 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:16:40.050 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:16:40.051 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:16:40.051 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:16:40.052 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:16:40.052 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:16:42.510 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 5.518 seconds (JVM running for 8.246)
2020-04-02 11:16:42.762 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 11:16:42.767 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:16:42.768 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:16:42.894 [ kafka-producer-network-thread | producer-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:17:51.423 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 14208 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 11:17:51.425 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 11:17:51.425 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 11:17:52.725 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 11:17:54.182 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:17:54.183 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:17:54.207 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:17:54.212 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:17:54.239 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:17:54.240 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:17:54.240 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 11:17:54.344 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:17:54.344 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:17:54.439 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:17:54.440 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:17:54.453 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:17:54.453 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:17:54.740 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:17:54.743 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:17:54.744 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:17:54.745 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:17:54.745 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:17:54.746 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:17:54.746 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:17:54.748 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:17:58.401 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 7.708 seconds (JVM running for 10.672)
2020-04-02 11:17:58.714 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 11:17:58.719 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:17:58.719 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:17:59.897 [ kafka-producer-network-thread | producer-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:21:06.611 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 14928 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 11:21:06.614 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 11:21:06.622 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 11:21:07.646 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 11:21:08.678 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:21:08.679 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 11:21:08.679 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:21:08.680 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 11:21:08.704 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:21:08.709 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 11:21:08.711 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 11:21:08.774 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:21:08.775 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:21:08.805 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:21:08.806 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:21:08.809 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 11:21:08.831 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 11:21:09.605 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:21:09.607 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 11:21:09.609 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:21:09.610 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 11:21:09.611 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:21:09.612 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:21:09.614 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 11:21:09.614 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 11:21:11.618 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.689 seconds (JVM running for 8.559)
2020-04-02 11:21:12.507 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:08:47.552 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 10012 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 14:08:47.554 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 14:08:47.555 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 14:08:48.596 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 14:08:49.745 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:08:49.746 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:08:49.748 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:08:49.748 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:08:49.774 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:08:49.781 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 14:08:49.781 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:08:49.885 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:08:49.885 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:08:49.916 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:08:49.916 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:08:49.975 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:08:49.975 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:08:50.857 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:08:50.861 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:08:50.864 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:08:50.865 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:08:50.866 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:08:50.866 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:08:50.868 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:08:50.868 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:08:53.052 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.241 seconds (JVM running for 9.703)
2020-04-02 14:08:54.006 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:19:26.450 [ main ] - [ INFO  ] [ c.lzycompany.messagequeue.MessagequeueApplication : 50 ] - Starting MessagequeueApplication on DESKTOP-2TI3SF3 with PID 15180 (E:\workSpace-idea\messagequeue\target\classes started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 14:19:26.455 [ main ] - [ DEBUG ] [ c.lzycompany.messagequeue.MessagequeueApplication : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 14:19:26.455 [ main ] - [ INFO  ] [ c.lzycompany.messagequeue.MessagequeueApplication : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 14:19:27.349 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 14:19:29.062 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Initializing ProtocolHandler ["http-nio-8082"]
2020-04-02 14:19:29.102 [ main ] - [ INFO  ] [ org.apache.catalina.core.StandardService : 173 ] - Starting service [Tomcat]
2020-04-02 14:19:29.103 [ main ] - [ INFO  ] [ org.apache.catalina.core.StandardEngine : 173 ] - Starting Servlet engine: [Apache Tomcat/9.0.16]
2020-04-02 14:19:29.110 [ main ] - [ INFO  ] [ org.apache.catalina.core.AprLifecycleListener : 173 ] - The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [E:\soft\JDK\JDK1.8\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;E:\soft\OracleServer\database\root\product\12.2.0\dbhome_1\bin;E:\soft\oracle\install\product\11.2.0\dbhome_1\bin;E:\soft\secureCRT\;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\iCLS\;C:\Program Files\Intel\Intel(R) Management Engine Components\iCLS\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;E:\soft\JDK\JDK1.8\bin;C:\Program Files\MySQL\MySQL Server 8.0\bin;E:\soft\apache-maven-3.0.5\apache-maven-3.0.5\apache-maven-3.0.5\bin;E:\soft\apache-tomcat-8.5.38\bin;E:\soft\apache-tomcat-8.5.38\lib;E:\soft\nodejs\;E:\soft\nodejs\globel;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\TortoiseSVN\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Intel\WiFi\bin\;C:\Program Files\Common Files\Intel\WirelessCommon\;E:\soft\GitHub\cmd;C:\Users\lzy\AppData\Local\Microsoft\WindowsApps;C:\Users\lzy\AppData\Roaming\npm;E:\soft\webStorm\WebStorm 2019.1.1\bin;C:\Users\lzy\AppData\Local\BypassRuntm;C:\Users\lzy\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Intel\WiFi\bin\;C:\Program Files\Common Files\Intel\WirelessCommon\;D:\soft\Fiddler2;.]
2020-04-02 14:19:29.303 [ main ] - [ INFO  ] [ o.a.c.core.ContainerBase.[Tomcat].[localhost].[/] : 173 ] - Initializing Spring embedded WebApplicationContext
2020-04-02 14:19:29.507 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:19:29.507 [ Thread-6 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:19:29.507 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:19:29.507 [ Thread-7 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:19:29.526 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:19:29.526 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 14:19:29.526 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:19:29.580 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:19:29.580 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:19:29.593 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:19:29.593 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:19:29.594 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:19:29.594 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:19:30.340 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:19:30.341 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:19:30.343 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:19:30.345 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:19:30.345 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:19:30.862 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Starting ProtocolHandler ["http-nio-8082"]
2020-04-02 14:19:30.867 [ main ] - [ ERROR ] [ org.apache.catalina.util.LifecycleBase : 175 ] - Failed to start component [Connector[HTTP/1.1-8082]]
org.apache.catalina.LifecycleException: Protocol handler start failed
	at org.apache.catalina.connector.Connector.startInternal(Connector.java:1008)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183)
	at org.apache.catalina.core.StandardService.addConnector(StandardService.java:226)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.addPreviouslyRemovedConnectors(TomcatWebServer.java:259)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.start(TomcatWebServer.java:197)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.startWebServer(ServletWebServerApplicationContext.java:311)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.finishRefresh(ServletWebServerApplicationContext.java:164)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:552)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:316)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248)
	at com.lzycompany.messagequeue.MessagequeueApplication.main(MessagequeueApplication.java:12)
Caused by: java.net.BindException: Address already in use: bind
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.tomcat.util.net.NioEndpoint.initServerSocket(NioEndpoint.java:236)
	at org.apache.tomcat.util.net.NioEndpoint.bind(NioEndpoint.java:210)
	at org.apache.tomcat.util.net.AbstractEndpoint.bindWithCleanup(AbstractEndpoint.java:1085)
	at org.apache.tomcat.util.net.AbstractEndpoint.start(AbstractEndpoint.java:1171)
	at org.apache.coyote.AbstractProtocol.start(AbstractProtocol.java:568)
	at org.apache.catalina.connector.Connector.startInternal(Connector.java:1005)
	... 14 common frames omitted
2020-04-02 14:19:30.873 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Pausing ProtocolHandler ["http-nio-8082"]
2020-04-02 14:19:30.874 [ main ] - [ INFO  ] [ org.apache.catalina.core.StandardService : 173 ] - Stopping service [Tomcat]
2020-04-02 14:19:30.977 [ main ] - [ INFO  ] [ org.apache.catalina.util.LifecycleBase : 173 ] - The stop() method was called on component [StandardServer[-1]] after stop() had already been called. The second call will be ignored.
2020-04-02 14:19:30.978 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Stopping ProtocolHandler ["http-nio-8082"]
2020-04-02 14:19:30.978 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Destroying ProtocolHandler ["http-nio-8082"]
2020-04-02 14:19:31.149 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:19:31.150 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:19:31.151 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:21:20.673 [ main ] - [ INFO  ] [ c.lzycompany.messagequeue.MessagequeueApplication : 50 ] - Starting MessagequeueApplication on DESKTOP-2TI3SF3 with PID 17280 (E:\workSpace-idea\messagequeue\target\classes started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 14:21:20.677 [ main ] - [ DEBUG ] [ c.lzycompany.messagequeue.MessagequeueApplication : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 14:21:20.678 [ main ] - [ INFO  ] [ c.lzycompany.messagequeue.MessagequeueApplication : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 14:21:21.495 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 14:21:22.475 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Initializing ProtocolHandler ["http-nio-8082"]
2020-04-02 14:21:22.484 [ main ] - [ INFO  ] [ org.apache.catalina.core.StandardService : 173 ] - Starting service [Tomcat]
2020-04-02 14:21:22.484 [ main ] - [ INFO  ] [ org.apache.catalina.core.StandardEngine : 173 ] - Starting Servlet engine: [Apache Tomcat/9.0.16]
2020-04-02 14:21:22.491 [ main ] - [ INFO  ] [ org.apache.catalina.core.AprLifecycleListener : 173 ] - The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [E:\soft\JDK\JDK1.8\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;E:\soft\OracleServer\database\root\product\12.2.0\dbhome_1\bin;E:\soft\oracle\install\product\11.2.0\dbhome_1\bin;E:\soft\secureCRT\;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\iCLS\;C:\Program Files\Intel\Intel(R) Management Engine Components\iCLS\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;E:\soft\JDK\JDK1.8\bin;C:\Program Files\MySQL\MySQL Server 8.0\bin;E:\soft\apache-maven-3.0.5\apache-maven-3.0.5\apache-maven-3.0.5\bin;E:\soft\apache-tomcat-8.5.38\bin;E:\soft\apache-tomcat-8.5.38\lib;E:\soft\nodejs\;E:\soft\nodejs\globel;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\TortoiseSVN\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Intel\WiFi\bin\;C:\Program Files\Common Files\Intel\WirelessCommon\;E:\soft\GitHub\cmd;C:\Users\lzy\AppData\Local\Microsoft\WindowsApps;C:\Users\lzy\AppData\Roaming\npm;E:\soft\webStorm\WebStorm 2019.1.1\bin;C:\Users\lzy\AppData\Local\BypassRuntm;C:\Users\lzy\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Intel\WiFi\bin\;C:\Program Files\Common Files\Intel\WirelessCommon\;D:\soft\Fiddler2;.]
2020-04-02 14:21:22.638 [ main ] - [ INFO  ] [ o.a.c.core.ContainerBase.[Tomcat].[localhost].[/] : 173 ] - Initializing Spring embedded WebApplicationContext
2020-04-02 14:21:22.813 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:21:22.813 [ Thread-6 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:21:22.813 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:21:22.814 [ Thread-7 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:21:22.828 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:21:22.828 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:21:22.828 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 14:21:22.878 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:21:22.878 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:21:22.891 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:21:22.892 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:21:22.892 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:21:22.892 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:21:23.945 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:21:23.948 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:21:23.950 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:21:23.950 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:21:24.178 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Starting ProtocolHandler ["http-nio-8082"]
2020-04-02 14:21:24.183 [ main ] - [ ERROR ] [ org.apache.catalina.util.LifecycleBase : 175 ] - Failed to start component [Connector[HTTP/1.1-8082]]
org.apache.catalina.LifecycleException: Protocol handler start failed
	at org.apache.catalina.connector.Connector.startInternal(Connector.java:1008)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183)
	at org.apache.catalina.core.StandardService.addConnector(StandardService.java:226)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.addPreviouslyRemovedConnectors(TomcatWebServer.java:259)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.start(TomcatWebServer.java:197)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.startWebServer(ServletWebServerApplicationContext.java:311)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.finishRefresh(ServletWebServerApplicationContext.java:164)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:552)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:316)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248)
	at com.lzycompany.messagequeue.MessagequeueApplication.main(MessagequeueApplication.java:12)
Caused by: java.net.BindException: Address already in use: bind
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.tomcat.util.net.NioEndpoint.initServerSocket(NioEndpoint.java:236)
	at org.apache.tomcat.util.net.NioEndpoint.bind(NioEndpoint.java:210)
	at org.apache.tomcat.util.net.AbstractEndpoint.bindWithCleanup(AbstractEndpoint.java:1085)
	at org.apache.tomcat.util.net.AbstractEndpoint.start(AbstractEndpoint.java:1171)
	at org.apache.coyote.AbstractProtocol.start(AbstractProtocol.java:568)
	at org.apache.catalina.connector.Connector.startInternal(Connector.java:1005)
	... 14 common frames omitted
2020-04-02 14:21:24.189 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Pausing ProtocolHandler ["http-nio-8082"]
2020-04-02 14:21:24.190 [ main ] - [ INFO  ] [ org.apache.catalina.core.StandardService : 173 ] - Stopping service [Tomcat]
2020-04-02 14:21:24.206 [ main ] - [ INFO  ] [ org.apache.catalina.util.LifecycleBase : 173 ] - The stop() method was called on component [StandardServer[-1]] after stop() had already been called. The second call will be ignored.
2020-04-02 14:21:24.206 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Stopping ProtocolHandler ["http-nio-8082"]
2020-04-02 14:21:24.207 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Destroying ProtocolHandler ["http-nio-8082"]
2020-04-02 14:21:24.835 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:21:24.835 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:21:24.837 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:21:24.837 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:21:58.016 [ main ] - [ INFO  ] [ c.lzycompany.messagequeue.MessagequeueApplication : 50 ] - Starting MessagequeueApplication on DESKTOP-2TI3SF3 with PID 5264 (E:\workSpace-idea\messagequeue\target\classes started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 14:21:58.019 [ main ] - [ DEBUG ] [ c.lzycompany.messagequeue.MessagequeueApplication : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 14:21:58.020 [ main ] - [ INFO  ] [ c.lzycompany.messagequeue.MessagequeueApplication : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 14:21:58.812 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 14:21:59.787 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Initializing ProtocolHandler ["http-nio-8080"]
2020-04-02 14:21:59.796 [ main ] - [ INFO  ] [ org.apache.catalina.core.StandardService : 173 ] - Starting service [Tomcat]
2020-04-02 14:21:59.796 [ main ] - [ INFO  ] [ org.apache.catalina.core.StandardEngine : 173 ] - Starting Servlet engine: [Apache Tomcat/9.0.16]
2020-04-02 14:21:59.804 [ main ] - [ INFO  ] [ org.apache.catalina.core.AprLifecycleListener : 173 ] - The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [E:\soft\JDK\JDK1.8\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;E:\soft\OracleServer\database\root\product\12.2.0\dbhome_1\bin;E:\soft\oracle\install\product\11.2.0\dbhome_1\bin;E:\soft\secureCRT\;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\iCLS\;C:\Program Files\Intel\Intel(R) Management Engine Components\iCLS\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;E:\soft\JDK\JDK1.8\bin;C:\Program Files\MySQL\MySQL Server 8.0\bin;E:\soft\apache-maven-3.0.5\apache-maven-3.0.5\apache-maven-3.0.5\bin;E:\soft\apache-tomcat-8.5.38\bin;E:\soft\apache-tomcat-8.5.38\lib;E:\soft\nodejs\;E:\soft\nodejs\globel;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\TortoiseSVN\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Intel\WiFi\bin\;C:\Program Files\Common Files\Intel\WirelessCommon\;E:\soft\GitHub\cmd;C:\Users\lzy\AppData\Local\Microsoft\WindowsApps;C:\Users\lzy\AppData\Roaming\npm;E:\soft\webStorm\WebStorm 2019.1.1\bin;C:\Users\lzy\AppData\Local\BypassRuntm;C:\Users\lzy\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Intel\WiFi\bin\;C:\Program Files\Common Files\Intel\WirelessCommon\;D:\soft\Fiddler2;.]
2020-04-02 14:21:59.947 [ main ] - [ INFO  ] [ o.a.c.core.ContainerBase.[Tomcat].[localhost].[/] : 173 ] - Initializing Spring embedded WebApplicationContext
2020-04-02 14:22:00.126 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:22:00.126 [ Thread-6 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:22:00.127 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:22:00.127 [ Thread-7 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:22:00.146 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:22:00.146 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 14:22:00.146 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:22:00.201 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:22:00.201 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:22:00.213 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:22:00.214 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:22:00.214 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:22:00.214 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:22:01.027 [ Thread-6 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:22:01.027 [ Thread-7 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:22:01.029 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:22:01.031 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:22:01.031 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:22:01.528 [ main ] - [ INFO  ] [ org.apache.coyote.http11.Http11NioProtocol : 173 ] - Starting ProtocolHandler ["http-nio-8080"]
2020-04-02 14:22:01.553 [ main ] - [ INFO  ] [ c.lzycompany.messagequeue.MessagequeueApplication : 59 ] - Started MessagequeueApplication in 4.331 seconds (JVM running for 6.195)
2020-04-02 14:22:01.947 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:22:01.948 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:22:01.948 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:22:31.937 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 729 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
2020-04-02 14:22:31.937 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 729 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
2020-04-02 14:22:34.934 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:34.935 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:37.929 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:38.180 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:41.208 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:41.365 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:50.273 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:51.948 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:53.834 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:55.401 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:57.863 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:22:59.216 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:02.358 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:06.639 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:09.167 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:10.628 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:13.272 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:14.579 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:17.393 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:18.521 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:22.817 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:26.838 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:30.544 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:30.995 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:34.889 [ Thread-6 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:35.282 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:39.198 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:43.563 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 671 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Connection to node 0 could not be established. Broker may not be available.
2020-04-02 14:23:46.576 [ Thread-7 ] - [ WARN  ] [ org.apache.kafka.clients.NetworkClient : 968 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Error while fetching metadata with correlation id 7 : {test0=LEADER_NOT_AVAILABLE}
2020-04-02 14:23:46.818 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:23:46.820 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:23:47.120 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:23:47.122 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:23:57.902 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 78
2020-04-02 14:23:57.904 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 78
2020-04-02 14:23:57.904 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 14:23:57.909 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 14:23:58.463 [ Thread-7 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============接受到消息了=============
2020-04-02 14:23:58.468 [ Thread-7 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 137 ] - ===把byte反序列化成对象异常===
java.io.StreamCorruptedException: invalid stream header: 32323232
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:866)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:358)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:132)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 14:23:58.468 [ Thread-7 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 91 ] - ===消费消息出错啦，德玛西亚！！！===
java.lang.NullPointerException: null
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:143)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 14:24:25.288 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 7248 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 14:24:25.290 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 14:24:25.290 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 14:24:26.318 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 14:24:27.411 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:24:27.411 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:24:27.412 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:24:27.412 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:24:27.436 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 14:24:27.435 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:24:27.441 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:24:27.513 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:24:27.513 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:24:27.561 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:24:27.562 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:24:27.567 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:24:27.567 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:24:28.402 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:24:28.406 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:24:28.410 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:24:28.410 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:24:28.403 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:24:28.412 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:24:28.413 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:24:28.414 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:24:30.622 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.98 seconds (JVM running for 8.886)
2020-04-02 14:24:31.257 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 14:24:31.333 [ kafka-coordinator-heartbeat-thread | test-consumer-group ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 14:24:31.483 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions [test0-0]
2020-04-02 14:24:31.483 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:24:31.548 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:24:31.931 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:24:31.931 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:24:32.387 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 79
2020-04-02 14:24:32.388 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 14:24:32.390 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 79
2020-04-02 14:24:32.394 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 14:24:32.867 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 79
2020-04-02 14:24:32.867 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 14:24:33.060 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 79
2020-04-02 14:24:33.062 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 14:24:33.891 [ Thread-7 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============接受到消息了=============
2020-04-02 14:24:33.894 [ Thread-7 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 165 ] - 执行消息serviceBeanName=sendMailService,methodName=sendMail
2020-04-02 14:24:33.894 [ Thread-7 ] - [ INFO  ] [ c.l.m.server.service.impl.SendMailServiceImpl : 34 ] - =====sendMail===== in params={subject=峡谷之巅见, receiveAccount=1209414113@qq.com, text=德玛西亚万岁！}
2020-04-02 14:24:41.246 [ Thread-7 ] - [ INFO  ] [ c.lzycompany.messagequeue.common.util.SendMailUtil : 43 ] - ===发送邮箱信息成功===
2020-04-02 14:24:41.247 [ Thread-7 ] - [ INFO  ] [ c.l.m.server.service.impl.SendMailServiceImpl : 46 ] - =====sendMail===== out 消耗时间=6633ms
2020-04-02 14:24:45.113 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 14:24:45.114 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:24:45.114 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:24:45.201 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 14:24:45.432 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions [test0-0]
2020-04-02 14:24:45.432 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:24:45.869 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 80
2020-04-02 14:24:45.869 [ Thread-6 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 14:24:45.870 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 80
2020-04-02 14:24:45.870 [ Thread-7 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 14:27:47.689 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 16024 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 14:27:47.691 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 14:27:47.691 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 14:27:48.554 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 14:27:49.474 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================kafka߳   111111  =============
2020-04-02 14:27:49.474 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================kafka߳   111111  =============
2020-04-02 14:27:49.474 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============kafka߳=============
2020-04-02 14:27:49.479 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============kafka߳=============
2020-04-02 14:27:49.497 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 14:27:49.497 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:27:49.497 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:27:50.416 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:27:50.417 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:27:50.444 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:27:50.454 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:27:50.467 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:27:50.467 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:27:51.250 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:27:51.252 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:27:51.254 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:27:51.256 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:27:51.256 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:27:52.104 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 82
2020-04-02 14:27:52.106 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 14:27:52.138 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 14:27:52.139 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 14:27:52.139 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:27:52.987 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 5.823 seconds (JVM running for 6.854)
2020-04-02 14:27:53.241 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 14:27:53.250 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:27:53.251 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:27:55.325 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 855 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Attempt to heartbeat failed since group is rebalancing
2020-04-02 14:27:55.538 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions [test0-0]
2020-04-02 14:27:55.539 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 14:27:55.616 [ kafka-producer-network-thread | producer-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:27:55.970 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 83
2020-04-02 14:27:55.971 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 14:27:55.972 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 83
2020-04-02 14:27:55.973 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 14:27:56.841 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============ܵϢ=============
2020-04-02 14:27:56.845 [ Thread-2 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 137 ] - ===byteлɶ쳣===
java.io.StreamCorruptedException: invalid stream header: 32323232
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:866)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:358)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:132)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 14:27:56.846 [ Thread-2 ] - [ ERROR ] [ c.l.m.server.consumer.KafkaConsumerRunner : 91 ] - ===Ϣǣ===
java.lang.NullPointerException: null
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.deserialize(KafkaConsumerRunner.java:143)
	at com.lzycompany.messagequeue.server.consumer.KafkaConsumerRunner.run(KafkaConsumerRunner.java:78)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 14:27:57.586 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 14:27:59.162 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 75 ] - ============ܵϢ=============
2020-04-02 14:27:59.163 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 165 ] - ִϢserviceBeanName=sendMailService,methodName=sendMail
2020-04-02 14:27:59.163 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.service.impl.SendMailServiceImpl : 34 ] - =====sendMail===== in params={subject=Ͽ֮ۼ, receiveAccount=1209414113@qq.com, text=꣡}
2020-04-02 14:28:07.422 [ Thread-2 ] - [ INFO  ] [ c.lzycompany.messagequeue.common.util.SendMailUtil : 43 ] - ===Ϣɹ===
2020-04-02 14:28:07.422 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.service.impl.SendMailServiceImpl : 46 ] - =====sendMail===== out ʱ=7572ms
2020-04-02 14:58:12.133 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 15056 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 14:58:12.135 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 14:58:12.137 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 14:58:13.271 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 14:58:14.410 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:58:14.411 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:58:14.412 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:58:14.414 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:58:14.441 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:58:14.447 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 14:58:14.449 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:58:14.520 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:58:14.521 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:58:14.544 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:58:14.545 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:58:14.551 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:58:14.552 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:58:18.123 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.656 seconds (JVM running for 9.535)
2020-04-02 14:59:18.386 [ main ] - [ ERROR ] [ c.l.m.server.producer.KafkaMessageProducer : 106 ] - ===执行同步发送kafka消息异常===
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
	at org.apache.kafka.clients.producer.KafkaProducer$FutureFailure.<init>(KafkaProducer.java:1186)
	at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:880)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:803)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:690)
	at com.lzycompany.messagequeue.server.producer.KafkaMessageProducer.sendAyn(KafkaMessageProducer.java:94)
	at com.lzycompany.messagequeue.server.producer.impl.KafkaMessageSendClient.sendAyn(KafkaMessageSendClient.java:38)
	at com.lzycompany.messagequeue.MessagequeueApplicationTests.contextLoads(MessagequeueApplicationTests.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.springframework.test.context.junit4.statements.RunBeforeTestExecutionCallbacks.evaluate(RunBeforeTestExecutionCallbacks.java:74)
	at org.springframework.test.context.junit4.statements.RunAfterTestExecutionCallbacks.evaluate(RunAfterTestExecutionCallbacks.java:84)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:251)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:97)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:70)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:190)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
2020-04-02 14:59:45.040 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 8572 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 14:59:45.042 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 14:59:45.043 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 14:59:46.505 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 14:59:47.807 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:59:47.808 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 14:59:47.810 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:59:47.812 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 14:59:47.835 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:59:47.835 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 14:59:47.835 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 14:59:48.755 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:59:48.756 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:59:48.786 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:59:48.786 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:59:48.803 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 14:59:48.803 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 14:59:51.213 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.664 seconds (JVM running for 7.907)
2020-04-02 15:00:44.514 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 50 ] - Starting KafkaProducerTest on DESKTOP-2TI3SF3 with PID 11068 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 15:00:44.550 [ main ] - [ DEBUG ] [ com.lzycompany.messagequeue.KafkaProducerTest : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 15:00:44.551 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 15:00:45.538 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 15:00:46.631 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:00:46.632 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:00:46.633 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:00:46.636 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:00:46.657 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:00:46.658 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:00:46.659 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 15:00:46.718 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:00:46.720 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:00:46.761 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:00:46.762 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:00:46.772 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:00:46.772 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:00:49.771 [ main ] - [ INFO  ] [ com.lzycompany.messagequeue.KafkaProducerTest : 59 ] - Started KafkaProducerTest in 5.883 seconds (JVM running for 8.534)
2020-04-02 15:00:50.036 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2020-04-02 15:00:50.045 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:00:50.046 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:17:13.336 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 3852 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 15:17:13.338 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 15:17:13.339 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 15:17:14.323 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 15:17:15.395 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:17:15.396 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:17:15.396 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:17:15.400 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:17:15.424 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:17:15.425 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:17:15.426 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 15:17:15.496 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:17:15.497 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:17:15.520 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:17:15.520 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:17:15.530 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:17:15.531 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:17:18.561 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.952 seconds (JVM running for 8.967)
2020-04-02 15:18:18.845 [ main ] - [ ERROR ] [ c.l.m.server.producer.KafkaMessageProducer : 106 ] - ===执行同步发送kafka消息异常===
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
	at org.apache.kafka.clients.producer.KafkaProducer$FutureFailure.<init>(KafkaProducer.java:1186)
	at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:880)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:803)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:690)
	at com.lzycompany.messagequeue.server.producer.KafkaMessageProducer.sendAyn(KafkaMessageProducer.java:94)
	at com.lzycompany.messagequeue.server.producer.impl.KafkaMessageSendClient.sendAyn(KafkaMessageSendClient.java:38)
	at com.lzycompany.messagequeue.MessagequeueApplicationTests.contextLoads(MessagequeueApplicationTests.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.springframework.test.context.junit4.statements.RunBeforeTestExecutionCallbacks.evaluate(RunBeforeTestExecutionCallbacks.java:74)
	at org.springframework.test.context.junit4.statements.RunAfterTestExecutionCallbacks.evaluate(RunAfterTestExecutionCallbacks.java:84)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:251)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:97)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:70)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:190)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
2020-04-02 15:19:36.746 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 17064 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 15:19:36.746 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 15:19:36.747 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 15:19:37.831 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 15:19:39.127 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:19:39.128 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:19:39.128 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:19:39.130 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:19:39.158 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:19:39.159 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:19:39.161 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 15:19:39.240 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:19:39.240 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:19:39.265 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:19:39.266 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:19:39.269 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:19:39.270 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:19:42.331 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.29 seconds (JVM running for 9.465)
2020-04-02 15:20:42.579 [ main ] - [ ERROR ] [ c.l.m.server.producer.KafkaMessageProducer : 106 ] - ===执行同步发送kafka消息异常===
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
	at org.apache.kafka.clients.producer.KafkaProducer$FutureFailure.<init>(KafkaProducer.java:1186)
	at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:880)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:803)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:690)
	at com.lzycompany.messagequeue.server.producer.KafkaMessageProducer.sendAyn(KafkaMessageProducer.java:94)
	at com.lzycompany.messagequeue.server.producer.impl.KafkaMessageSendClient.sendAyn(KafkaMessageSendClient.java:38)
	at com.lzycompany.messagequeue.MessagequeueApplicationTests.contextLoads(MessagequeueApplicationTests.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.springframework.test.context.junit4.statements.RunBeforeTestExecutionCallbacks.evaluate(RunBeforeTestExecutionCallbacks.java:74)
	at org.springframework.test.context.junit4.statements.RunAfterTestExecutionCallbacks.evaluate(RunAfterTestExecutionCallbacks.java:84)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:251)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:97)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:70)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:190)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
2020-04-02 15:49:01.365 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 10044 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 15:49:01.367 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 15:49:01.368 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 15:49:02.312 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 15:49:03.415 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:49:03.416 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:49:03.416 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:49:03.419 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:49:03.445 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 15:49:03.446 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:49:03.446 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:49:03.516 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:49:03.516 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:49:03.540 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:49:03.544 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:49:03.546 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:49:03.546 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:49:07.520 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.898 seconds (JVM running for 9.621)
2020-04-02 15:50:07.799 [ main ] - [ ERROR ] [ c.l.m.server.producer.KafkaMessageProducer : 106 ] - ===执行同步发送kafka消息异常===
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
	at org.apache.kafka.clients.producer.KafkaProducer$FutureFailure.<init>(KafkaProducer.java:1186)
	at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:880)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:803)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:690)
	at com.lzycompany.messagequeue.server.producer.KafkaMessageProducer.sendAyn(KafkaMessageProducer.java:94)
	at com.lzycompany.messagequeue.server.producer.impl.KafkaMessageSendClient.sendAyn(KafkaMessageSendClient.java:38)
	at com.lzycompany.messagequeue.MessagequeueApplicationTests.contextLoads(MessagequeueApplicationTests.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.springframework.test.context.junit4.statements.RunBeforeTestExecutionCallbacks.evaluate(RunBeforeTestExecutionCallbacks.java:74)
	at org.springframework.test.context.junit4.statements.RunAfterTestExecutionCallbacks.evaluate(RunAfterTestExecutionCallbacks.java:84)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:251)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:97)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:70)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:190)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
2020-04-02 15:56:04.316 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 4508 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 15:56:04.318 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 15:56:04.318 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 15:56:05.379 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 15:56:06.457 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:56:06.458 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 15:56:06.458 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:56:06.460 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 15:56:06.486 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:56:06.487 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 15:56:06.489 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 15:56:06.552 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:56:06.552 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:56:06.572 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:56:06.572 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:56:06.573 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 15:56:06.574 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 15:56:09.873 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.201 seconds (JVM running for 8.842)
2020-04-02 16:15:18.164 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 7224 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 16:15:18.166 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 16:15:18.167 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 16:15:19.147 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 16:15:20.247 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:15:20.247 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:15:20.248 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:15:20.248 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:15:20.276 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:15:20.277 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 16:15:20.278 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:15:20.347 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:15:20.347 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:15:20.380 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:15:20.380 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:15:20.382 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:15:20.382 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:15:20.675 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:15:20.675 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:15:20.678 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:15:20.678 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:15:20.681 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:15:20.681 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:15:20.681 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:15:20.683 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:15:20.965 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 85
2020-04-02 16:15:20.966 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 16:15:20.970 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 85
2020-04-02 16:15:20.972 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 16:15:23.494 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.179 seconds (JVM running for 8.955)
2020-04-02 16:15:23.872 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:17:32.227 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 9676 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 16:17:32.230 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 16:17:32.231 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 16:17:33.158 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 16:17:34.388 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:17:34.389 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:17:34.389 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:17:34.390 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:17:34.416 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 16:17:34.417 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:17:34.419 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:17:34.482 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:17:34.482 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:17:34.516 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:17:34.516 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:17:34.521 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:17:34.521 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:17:37.535 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.997 seconds (JVM running for 8.585)
2020-04-02 16:25:32.874 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 11092 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 16:25:32.876 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 16:25:32.877 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 16:25:33.819 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 16:25:34.858 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:25:34.858 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:25:34.867 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:25:34.869 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:25:34.893 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:25:34.894 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 16:25:34.896 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:25:34.959 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:25:34.961 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:25:34.990 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:25:34.990 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:25:34.998 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:25:34.999 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:25:35.820 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:25:35.824 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:25:35.826 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:25:35.826 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:25:35.822 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:25:35.830 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:25:35.831 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:25:35.832 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:25:37.577 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 90
2020-04-02 16:25:37.578 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 16:25:37.592 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 90
2020-04-02 16:25:37.593 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 16:25:37.915 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.886 seconds (JVM running for 8.597)
2020-04-02 16:25:38.801 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:28:31.631 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 14724 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 16:28:31.632 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 16:28:31.633 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 16:28:32.565 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 16:28:33.633 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:28:33.634 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:28:33.634 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:28:33.636 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:28:33.658 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:28:33.658 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 16:28:33.659 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:28:33.724 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:28:33.724 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:28:33.745 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:28:33.745 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:28:33.747 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:28:33.747 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:28:34.163 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:28:34.166 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:28:34.168 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:28:34.169 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:28:34.560 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:28:34.562 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:28:34.563 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:28:34.563 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:28:36.519 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.7 seconds (JVM running for 8.37)
2020-04-02 16:28:37.114 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:28:37.294 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 92
2020-04-02 16:28:37.295 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 16:35:10.402 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 17140 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 16:35:10.404 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 16:35:10.404 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 16:35:11.419 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 16:35:12.605 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:35:12.605 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:35:12.606 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:35:12.619 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:35:12.632 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 16:35:12.633 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:35:12.633 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:35:12.698 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:35:12.699 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:35:12.721 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:35:12.721 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:35:12.723 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:35:12.723 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:35:13.242 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:35:13.245 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:35:13.248 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:35:13.248 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:35:13.395 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:35:13.395 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:35:13.396 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:35:13.397 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:35:15.492 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 94
2020-04-02 16:35:15.494 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 16:35:15.499 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 94
2020-04-02 16:35:15.499 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 16:35:15.801 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 6.028 seconds (JVM running for 8.676)
2020-04-02 16:35:16.400 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:38:57.945 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 15468 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 16:38:57.947 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 16:38:57.948 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 16:38:58.865 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 16:38:59.934 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:38:59.934 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:38:59.935 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:38:59.935 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:38:59.963 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:38:59.963 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 16:38:59.963 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:39:00.023 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:39:00.023 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:39:00.055 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:39:00.056 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:39:00.057 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:39:00.057 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:39:02.819 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.549 seconds (JVM running for 8.149)
2020-04-02 16:40:03.075 [ main ] - [ ERROR ] [ c.l.m.server.producer.KafkaMessageProducer : 106 ] - ===执行同步发送kafka消息异常===
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
	at org.apache.kafka.clients.producer.KafkaProducer$FutureFailure.<init>(KafkaProducer.java:1186)
	at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:880)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:803)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:690)
	at com.lzycompany.messagequeue.server.producer.KafkaMessageProducer.sendAyn(KafkaMessageProducer.java:94)
	at com.lzycompany.messagequeue.server.producer.impl.KafkaMessageSendClient.sendAyn(KafkaMessageSendClient.java:38)
	at com.lzycompany.messagequeue.MessagequeueApplicationTests.contextLoads(MessagequeueApplicationTests.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.springframework.test.context.junit4.statements.RunBeforeTestExecutionCallbacks.evaluate(RunBeforeTestExecutionCallbacks.java:74)
	at org.springframework.test.context.junit4.statements.RunAfterTestExecutionCallbacks.evaluate(RunAfterTestExecutionCallbacks.java:84)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:251)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:97)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:70)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:190)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
2020-04-02 16:40:28.107 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 12780 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 16:40:28.108 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 16:40:28.109 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 16:40:29.015 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 16:40:30.034 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:40:30.034 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:40:30.035 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 16:40:30.037 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 16:40:30.059 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 16:40:30.059 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:40:30.060 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 16:40:30.121 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:40:30.122 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:40:30.147 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:40:30.148 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:40:30.155 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 16:40:30.155 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 16:40:30.716 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:40:30.719 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:40:30.722 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:40:30.722 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:40:31.056 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 16:40:31.060 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 16:40:31.061 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 16:40:31.061 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 16:40:32.132 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 96
2020-04-02 16:40:32.135 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 96
2020-04-02 16:40:32.152 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 16:40:32.155 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 16:40:33.312 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 5.95 seconds (JVM running for 8.628)
2020-04-02 16:40:33.751 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 17:02:01.422 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 50 ] - Starting MessagequeueApplicationTests on DESKTOP-2TI3SF3 with PID 6256 (started by lzy in E:\workSpace-idea\messagequeue)
2020-04-02 17:02:01.424 [ main ] - [ DEBUG ] [ c.l.messagequeue.MessagequeueApplicationTests : 53 ] - Running with Spring Boot v2.1.3.RELEASE, Spring v5.1.5.RELEASE
2020-04-02 17:02:01.425 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 675 ] - No active profile set, falling back to default profiles: default
2020-04-02 17:02:02.416 [ main ] - [ WARN  ] [ org.mybatis.spring.mapper.ClassPathMapperScanner : 44 ] - No MyBatis mapper was found in '[com.lzycompany.messagequeue]' package. Please check your configuration.
2020-04-02 17:02:03.585 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 17:02:03.585 [ Thread-2 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 17:02:03.586 [ main ] - [ INFO  ] [ c.l.m.server.consumer.impl.KafkaConsumerServer : 45 ] - ==================启动了kafka消费线程   111111  =============
2020-04-02 17:02:03.589 [ Thread-3 ] - [ INFO  ] [ c.l.m.server.consumer.KafkaConsumerRunner : 67 ] - =============启动了kafka消费线程=============
2020-04-02 17:02:03.616 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 17:02:03.617 [ main ] - [ INFO  ] [ org.apache.kafka.clients.producer.ProducerConfig : 279 ] - ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [114.215.179.123:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-04-02 17:02:03.616 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.consumer.ConsumerConfig : 279 ] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [114.215.179.123:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-04-02 17:02:03.720 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 17:02:03.720 [ main ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 17:02:03.772 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 17:02:03.773 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 17:02:03.799 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 109 ] - Kafka version : 2.0.1
2020-04-02 17:02:03.801 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.common.utils.AppInfoParser : 110 ] - Kafka commitId : fa14705e51bd2ce5
2020-04-02 17:02:04.138 [ Thread-2 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 17:02:04.132 [ Thread-3 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
2020-04-02 17:02:04.142 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 17:02:04.142 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 677 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator 114.215.179.123:9092 (id: 2147483647 rack: null)
2020-04-02 17:02:04.144 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 17:02:04.145 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] (Re-)joining group
2020-04-02 17:02:04.148 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 472 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2020-04-02 17:02:04.148 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 509 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2020-04-02 17:02:05.547 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Successfully joined group with generation 101
2020-04-02 17:02:05.547 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.AbstractCoordinator : 473 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 101
2020-04-02 17:02:05.550 [ Thread-2 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-1, groupId=test-consumer-group] Setting newly assigned partitions [test0-0]
2020-04-02 17:02:05.555 [ Thread-3 ] - [ INFO  ] [ o.a.k.c.consumer.internals.ConsumerCoordinator : 280 ] - [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions []
2020-04-02 17:02:07.954 [ main ] - [ INFO  ] [ c.l.messagequeue.MessagequeueApplicationTests : 59 ] - Started MessagequeueApplicationTests in 7.227 seconds (JVM running for 10.086)
2020-04-02 17:02:08.342 [ kafka-producer-network-thread | producer-1 ] - [ INFO  ] [ org.apache.kafka.clients.Metadata : 285 ] - Cluster ID: FqHzfOZFSqWZOuE9SqD1nA
